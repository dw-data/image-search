{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the labels collected using Google Cloud's Natural Language API don't seem very useful and meaningful, we will try to use a simpler approach: counting specific keywords that may indicate that the content is sexualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rodrigo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/rodrigo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../output/dataset/complete-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.961432\n",
       "True     0.038568\n",
       "Name: full_title, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many % of the requests failed to retrieve title data?\n",
    "df.full_title.isna().value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the nans with an empty string\n",
    "df[\"full_title\"] = df.full_title.fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds labels\n",
    "df[\"racy_bool\"] = df.racy.apply(lambda x: x in [\"LIKELY\", \"VERY_LIKELY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are we looking for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to find specific keywords that may indicate sexualized content. \n",
    "\n",
    "This list was made initially with a short set of words that I devised from doing anedoctal Google searches, as well as with words that are more common at entries with racy images than in others. \n",
    "\n",
    "It was later expanded iteratively, as I noticed that more words could be useful for detecting objetifying websites.\n",
    "\n",
    "Notice that we are looking both at the titles shown on Google images and at the domains in which the images are hosted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's find which words are more likely to appear in titles of images marked as likely/unlikely to be racy, when compared to other images. To do so, we will use something that is similar to a tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textblob_tokenizer(str_input, min_chars=2):\n",
    "    '''\n",
    "    A TextBlob tokenizer that splits a string\n",
    "    into  words. Returns a list of words.\n",
    "    \n",
    "    >> Params\n",
    "    \n",
    "    str_input -> The text block to be tokenized\n",
    "    min_chars -> How many characters a world should have to be included. Default: 2\n",
    "    '''\n",
    "   \n",
    "    blob = TextBlob(str_input.lower())\n",
    "    \n",
    "    tokens = blob.words\n",
    "   \n",
    "    # We will ignore numbers and short words\n",
    "    words = [token for token in tokens if len(token) > min_chars and not token.isnumeric()] \n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrix(df, column):\n",
    "    '''\n",
    "    Creates a numeric matrix using the text entries of the\n",
    "    dataframe and the previously defined tokenizer.\n",
    "    \n",
    "    >> Params\n",
    "    \n",
    "    df -> The dataframe containing the text that is to be tokenized\n",
    "    column -> The column in the dataframe where the text is located\n",
    "    '''\n",
    "    \n",
    "\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "        \n",
    "    vec = CountVectorizer(tokenizer=textblob_tokenizer, stop_words=stop_words)\n",
    "    \n",
    "    matrix = vec.fit_transform(df[column])\n",
    "    \n",
    "    matrix = pd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())\n",
    "\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose(df):\n",
    "    '''\n",
    "    Uses the built-in pandas method to transpose\n",
    "    the axes of the dataframe passed as a parameter.\n",
    "    \n",
    "    >> Params\n",
    "    \n",
    "    df -> The dataframe to be transposed\n",
    "    '''\n",
    "        \n",
    "    return df.transpose().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sums(df):\n",
    "    '''\n",
    "    The function counts how many times a word has \n",
    "    been used on the dataframe. It is an envelope for\n",
    "    the df.sum() method, also built-in in Pandas.\n",
    "    \n",
    "    >> Params\n",
    "    \n",
    "    df -> The dataframe with the words that must be added.\n",
    "    '''\n",
    "        \n",
    "    df['WORD_SUM'] = df.sum(axis=1)\n",
    "    \n",
    "    # Esse valor será o mesmo ao longo de todo o dataframe\n",
    "    df['TOTAL_SUM'] = df.WORD_SUM.sum(axis=0)\n",
    "    \n",
    "    # Mantém apenas as colunas de interesse\n",
    "    df = df[['index', 'WORD_SUM', 'TOTAL_SUM']]\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratios(df, multiplier=100):\n",
    "    '''\n",
    "    Based on the sums obtained get_sum(df), this function\n",
    "    calculates the usage ratio of each word.\n",
    "    \n",
    "    \n",
    "    >> Params\n",
    "    \n",
    "    df -> The dataframe with word count information\n",
    "    multiplier -> An integer that will be the multiplier of the ratio.\n",
    "    For example, 1 occurrence for every 10,000 or 20,000 words. Default: 100\n",
    "    '''\n",
    "        \n",
    "    df['WORD_RATIO'] = (df['WORD_SUM'] / df['TOTAL_SUM']) * multiplier\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dfs(racy_df, not_racy_df):\n",
    "    '''\n",
    "    Merges the dataframe with the tokens and ratios for\n",
    "    the racy and not racy content into a single dataframe.\n",
    "    \n",
    "    \n",
    "    >> Params\n",
    "    \n",
    "    racy_df-> the df on the left of merge, with the\n",
    "    ratios for each word in images tagged as racy\n",
    "    \n",
    "    not_racy_df -> the df on the right, with ratios\n",
    "    for each word in all remaining images\n",
    "    '''\n",
    "    \n",
    "    suffixes = [ \"_RACY\", \"_NOT_RACY\" ]\n",
    "    \n",
    "    # Merges both dataframes on the tokens\n",
    "    df = racy_df.merge(not_racy_df, on='index', how='outer', suffixes=suffixes)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilities(df):\n",
    "    '''\n",
    "    Calculate the difference between the usage ratios\n",
    "    of a given word in content that is marked as racy\n",
    "    and in content that is not. That is, here we find\n",
    "    the words that are more associated with racy pics.\n",
    "    \n",
    "    >> Params\n",
    "    \n",
    "    df -> The df resulting from merge_dfs()   \n",
    "    '''\n",
    "        \n",
    "    df['TIMES_MORE_LIKELY_RACY']  = df[\"WORD_RATIO_RACY\"] / df[\"WORD_RATIO_NOT_RACY\"]\n",
    "    df['TIMES_MORE_LIKELY_NOT_RACY'] = df[\"WORD_RATIO_NOT_RACY\"] / df[\"WORD_RATIO_RACY\"]\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(df):\n",
    "    '''\n",
    "    Wrapper function that calls all the functions\n",
    "    defined above and returns a df with the words\n",
    "    that are more likely to appear in racy content.\n",
    "    '''\n",
    "    \n",
    "    # Creates two different dfs\n",
    "    racy_df = df[df.racy_bool]\n",
    "    not_racy_df = df[~df.racy_bool]\n",
    "    \n",
    "    racy_df = make_matrix(racy_df, \"title\")\n",
    "    not_racy_df = make_matrix(not_racy_df, \"title\")\n",
    "        \n",
    "    racy_df = transpose(racy_df)\n",
    "    not_racy_df = transpose(not_racy_df)\n",
    "            \n",
    "    racy_df = get_sums(racy_df)\n",
    "    not_racy_df = get_sums(not_racy_df)\n",
    "    \n",
    "    # Keep only words with a minimum frequency\n",
    "    racy_df = racy_df[racy_df.WORD_SUM > 10]\n",
    "    not_racy_df = not_racy_df[not_racy_df.WORD_SUM > 10]\n",
    "\n",
    "    \n",
    "    racy_df = get_ratios(racy_df, 10000)\n",
    "    not_racy_df = get_ratios(not_racy_df, 10000)\n",
    "        \n",
    "    results = merge_dfs(racy_df, not_racy_df)\n",
    "    \n",
    "    results = get_probabilities(results)\n",
    "    \n",
    "    # Round\n",
    "    results = results.round(decimals=5)\n",
    "\n",
    "    # Removes words that weren't present in both dfs\n",
    "    results = results.dropna()\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 9.78 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = calculate(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which words are more likely to be associated with racy pictures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results.sort_values(by=\"TIMES_MORE_LIKELY_RACY\", ascending=False).head(100)[[\"index\", \"TIMES_MORE_LIKELY_RACY\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build our list of words to look for with some of those, filtering out the words that indicate nationality and expanding it with some words variations of those terms. We also didn't use 'beauty' or 'beautiful' because, although associated with racy pictures, those words were producing lots of false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    \"sexy\", \"hot\", \"hottest\",\n",
    "    #\"beauty\", \"beautiful\",\n",
    "    \"sex\", \"laid\", \"fuck\",\n",
    "    \"marry\", \"marriage\", \"bride\", \"brides\", \"wife\", \"wives\", \"mail\", \"order\", \n",
    "    \"dating\", \"date\", \"meet\", \"single\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds word boundaries:\n",
    "keywords = [r\"\\b\" + word + r\"\\b\" for word in keywords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can mark all the titles that contain such words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = re.compile(\"|\".join(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"keywords_bool\"] = df.full_title.str.lower().apply(lambda x: True if capture.search(x) else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    17792\n",
       "True      2769\n",
       "Name: keywords_bool, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many entries have the keyword?\n",
    "df.keywords_bool.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output\n",
    "df.to_csv(\"../output/dataset/complete-data-classified.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
